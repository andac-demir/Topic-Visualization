{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Vis Midterm.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andac-demir/Topic-Visualization/blob/master/topicVis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNrg0Sx-VE6d",
        "colab_type": "text"
      },
      "source": [
        "## Import Necessary Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBdrG37QTgR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scikit Learn Dataset Loader\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Natural Language Tookit\n",
        "import nltk\n",
        "\n",
        "# Stop words list from NLTK\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Tokenizer from NLTK\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Lemmatizer and Stemmer from NLTK\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# List of punctuation from string\n",
        "from string import punctuation\n",
        "\n",
        "# Numpy/plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "# Pandas\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sKvJhOjWHmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run NLTK download if not run yet\n",
        "nltk.download(info_or_id=\"popular\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ7RfG9tVLko",
        "colab_type": "text"
      },
      "source": [
        "# Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z-Lk81aUX4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = fetch_20newsgroups(subset='all', shuffle=True)\n",
        "dataset_body_only = fetch_20newsgroups(subset='all', shuffle=True, remove=('headers', 'footers', 'quotes'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us2TAplHsMfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.data[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C01z4SnDVOGQ",
        "colab_type": "text"
      },
      "source": [
        "# Part 1\n",
        "Preprocess 20 Newsgroup dataset as corpus and visualize its statistical information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK-QXAVWU3At",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility functions for cleaning up text\n",
        "# Note: All apply to the same level of text (either a string of words of list of tokenized words)\n",
        "import itertools\n",
        "def flatmap(f, items):\n",
        "  return itertools.chain.from_iterable(map(f, items))\n",
        "\n",
        "import re\n",
        "\n",
        "def remove_emails_and_hostnames(text):\n",
        "  return re.sub('(\\S+@\\S+|\\S+\\.\\S+\\.\\S+\\.\\S+)', '', text)\n",
        "\n",
        "def split_by_sentence(text):\n",
        "  return text.split('.')\n",
        "\n",
        "def split_by_line(text):\n",
        "  return text.splitlines()\n",
        "\n",
        "def remove_punctuation(text):\n",
        "  return \"\".join([c for c in text if c not in punctuation])\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
        "def tokenize(text):\n",
        "  return tokenizer.tokenize(text)\n",
        "\n",
        "def lower(text):\n",
        "  return text.lower()\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  return filter(lambda w : w not in stopwords.words('english'), text)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize(text):\n",
        "  return map(lemmatizer.lemmatize, text)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "def stem(text):\n",
        "  return map(stemmer.stem, text)\n",
        "\n",
        "def rejoin_words(text):\n",
        "  return \" \".join(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E6FnkpcsRGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rejoin_words(lemmatize(remove_stopwords(tokenize(lower(remove_punctuation(dataset.data[0]))))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFRRKkS6ZYPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply text cleanup to bundle, in order\n",
        "def cleanup_text(text):\n",
        "  return rejoin_words(lemmatize(remove_stopwords(tokenize(lower(remove_punctuation(text))))))\n",
        "\n",
        "def cleanup_text_stemmer(text):\n",
        "  return rejoin_words(stem(remove_stopwords(tokenize(lower(remove_punctuation(text))))))\n",
        "\n",
        "cleaned_dataset_by_document_iter = map(cleanup_text, map(remove_emails_and_hostnames, dataset.data))\n",
        "cleaned_dataset_by_sentence_iter = filter(lambda s : s and not s.isspace(), map(cleanup_text, flatmap(split_by_sentence, map(remove_emails_and_hostnames, dataset.data))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb9WsaR5ajU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compare cleaned and uncleaned datasets!\n",
        "\n",
        "#print(\"ORIGINAL:\")\n",
        "#print(repr(dataset.data[0]))\n",
        "#print(\"CLEANED:\")\n",
        "#print(next(cleaned_dataset_by_document_iter))\n",
        "#print(\"CLEANED BY SENTENCE:\")\n",
        "#print(next(cleaned_dataset_by_sentence_iter))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pVrjUKzy8Zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load sentence length data into memory\n",
        "sentence_lengths_iter = map(len, map(lambda s : s.split(\" \"), cleaned_dataset_by_sentence_iter))\n",
        "sentence_lengths_array = np.fromiter(sentence_lengths_iter, dtype=int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0cmbgF1R5J7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filter out outliers\n",
        "avg = np.average(sentence_lengths_array)\n",
        "std_dev = np.std(sentence_lengths_array)\n",
        "keep_condition = np.abs(sentence_lengths_array - avg) < 2*std_dev\n",
        "sentence_lengths_array_filtered = np.extract(keep_condition, sentence_lengths_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC-RTUFtRrgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Describe sentence length distribution\n",
        "scipy.stats.describe(sentence_lengths_array_filtered)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwYMhqspRwIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sentence length histogram\n",
        "plt.figure(figsize=(16, 12))\n",
        "n, bins, patches = plt.hist(sentence_lengths_array_filtered, bins=np.max(sentence_lengths_array_filtered))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WGNH_klDgGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove sentence_lengths_array from memory\n",
        "del(sentence_lengths_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ffCYd_rwQOQ",
        "colab_type": "text"
      },
      "source": [
        "# Part 2\n",
        "Build two different vocabularies upon different preprocessing ways; Learn Bag-of-words (BoW) and TF-IDF model with each vocabulary accordingly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxSyk1AewPmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define two different cleaned corpuses\n",
        "corpus1_iter = map(cleanup_text, map(remove_emails_and_hostnames, dataset.data))\n",
        "corpus2_iter = map(cleanup_text, map(remove_emails_and_hostnames, dataset_body_only.data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUKHz03_x6GP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bag-Of-Words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer1 = CountVectorizer()\n",
        "bow1 = vectorizer1.fit_transform(corpus1_iter)\n",
        "\n",
        "vectorizer2 = CountVectorizer()\n",
        "bow2 = vectorizer2.fit_transform(corpus2_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLAo_TSEkrGy",
        "colab_type": "text"
      },
      "source": [
        "# Bag-Of-Words 1 Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob3EQmsg5O2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Vocab length: {}\".format(len(vectorizer1.vocabulary_)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9gffUDo5WxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_MOST_COMMON = 200\n",
        "counts = np.sort(np.sum(bow1, axis=0))[0,::-1][0,0:N_MOST_COMMON].transpose()\n",
        "bar_positions = np.arange(len(counts))\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.plot(bar_positions, counts)\n",
        "plt.ylabel(\"# of occurences\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2wayznHa5UB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Most common words:\n",
        "indices = np.sum(bow1, axis=0).argsort().transpose()[-10:][::-1]\n",
        "print(np.array(vectorizer1.get_feature_names())[indices])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAvJgH5Dkwqp",
        "colab_type": "text"
      },
      "source": [
        "# Bag-Of-Words 2 Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRlFpY_wk1Yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Vocab length: {}\".format(len(vectorizer2.vocabulary_)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaVTn5uFk_yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_MOST_COMMON = 200\n",
        "counts = np.sort(np.sum(bow2, axis=0))[0,::-1][0,0:N_MOST_COMMON].transpose()\n",
        "bar_positions = np.arange(len(counts))\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.plot(bar_positions, counts)\n",
        "plt.ylabel(\"# of occurences\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzQDsdxxlEAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Most common words:\n",
        "indices = np.sum(bow2, axis=0).argsort().transpose()[-20:][::-1]\n",
        "print(np.array(vectorizer2.get_feature_names())[indices])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0CXxXdnyM2U",
        "colab_type": "text"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILY0H6SYyLqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ0IZNXRxMK8",
        "colab_type": "text"
      },
      "source": [
        "## TF-IDF 1 Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSaaxbEpxPL3",
        "colab_type": "text"
      },
      "source": [
        "## TF-IDF 2 Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfArmJKByH1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer2 = TfidfTransformer()\n",
        "tf_idf2 = transformer2.fit_transform(bow2)\n",
        "idf2_df = pd.DataFrame(transformer2.idf_, index=vectorizer2.get_feature_names(),columns=[\"idf_weights\"])\n",
        "idf2_df.sort_values(by=['idf_weights']).head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar8sT2ar71to",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TF_IDF info for each document is in tf_idf2\n",
        "# Looking at first document:\n",
        "print(dataset_body_only.data[0])\n",
        "\n",
        "first_document_tf_idf_df2 = pd.DataFrame(tf_idf2[0].T.todense(), index=vectorizer2.get_feature_names(), columns=[\"tf-idf value\"])\n",
        "first_document_tf_idf_df2.sort_values(by=[\"tf-idf value\"], ascending=False).head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVTrbFIm87Fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Looking at the second document\n",
        "print(dataset_body_only.data[1])\n",
        "\n",
        "second_document_tf_idf_df2 = pd.DataFrame(tf_idf2[1].T.todense(), index=vectorizer2.get_feature_names(), columns=[\"tf-idf value\"])\n",
        "second_document_tf_idf_df2.sort_values(by=[\"tf-idf value\"], ascending=False).head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C1feKo89sKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_words = []\n",
        "for i, name in enumerate(dataset_body_only.target_names):\n",
        "  top_words.append(np.array(vectorizer2.get_feature_names())[tf_idf2[dataset_body_only.target == i].mean(axis=0).argsort().T[-10:][::-1]])\n",
        "\n",
        "top_words_df = pd.DataFrame(np.array(top_words).squeeze().T, columns=dataset_body_only.target_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0E_8FVKB8nA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_words_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvUiDtmes0Sl",
        "colab_type": "text"
      },
      "source": [
        "# Part 3 LDA Models\n",
        "\n",
        "Train two LDA models upon the vocabularies in Step 2; Visualize topics with four different\n",
        "methods; and eventually get the topic distribution (as feature) for each document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bpHkOXfhBK4",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. LDA Model with Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcsfDnwIyI88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define two different cleaned corpuses\n",
        "corpus1_iter = map(cleanup_text, map(remove_emails_and_hostnames, dataset.data))\n",
        "corpus2_iter = map(cleanup_text, map(remove_emails_and_hostnames, dataset_body_only.data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beKmsb6BUS9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lemmatized = []\n",
        "for _, Str in enumerate(corpus2_iter):\n",
        "  data_lemmatized.append(Str.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8UkYXa7bT-W_",
        "colab": {}
      },
      "source": [
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models import CoherenceModel\n",
        "import gensim.corpora as corpora\n",
        "!pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "from pyLDAvis.gensim import prepare\n",
        "import warnings\n",
        "\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "# filter out tokens that appear in less than 15 documents:\n",
        "id2word.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
        "bow_corpus = [id2word.doc2bow(doc) for doc in data_lemmatized]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd1gcWD0lE4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import LdaMulticore\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "  warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "  lda_model_bow = LdaMulticore(bow_corpus, num_topics=10, \n",
        "                               id2word=id2word, passes=2, workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WyqtwyDpYb3",
        "colab_type": "text"
      },
      "source": [
        "## PyLDAVis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQJ5eaYzmWPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize the information contained in the topic model\n",
        "\n",
        "import pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model_bow, bow_corpus, dictionary=id2word)\n",
        "vis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lC9W6bEplvb",
        "colab_type": "text"
      },
      "source": [
        "## t-SNE Clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9rbv2sppz_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get topic weights and dominant topics ------------\n",
        "from sklearn.manifold import TSNE\n",
        "from bokeh.plotting import figure, output_file, show\n",
        "from bokeh.models import Label\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "# Get topic weights\n",
        "topic_weights = []\n",
        "for i, row_list in enumerate(lda_model_bow[bow_corpus]):\n",
        "    topic_weights.append([w for i, w in row_list[0]])\n",
        "\n",
        "# Array of topic weights    \n",
        "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
        "\n",
        "# Keep the well separated points (optional)\n",
        "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
        "\n",
        "# Dominant topic number in each doc\n",
        "topic_num = np.argmax(arr, axis=1)\n",
        "\n",
        "# tSNE Dimension Reduction\n",
        "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
        "tsne_lda = tsne_model.fit_transform(arr)\n",
        "\n",
        "# Plot the Topic Clusters using Bokeh\n",
        "output_notebook()\n",
        "n_topics = 10\n",
        "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
        "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
        "              plot_width=900, plot_height=700)\n",
        "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
        "show(plot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqFEUdp9sIKV",
        "colab_type": "text"
      },
      "source": [
        "## Word Clouds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7SbeuZ5sKFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Wordcloud of Top N words in each topic\n",
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.colors as mcolors\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', \n",
        "                   'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', \n",
        "                   'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', \n",
        "                   'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', \n",
        "                   'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
        "\n",
        "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
        "\n",
        "cloud = WordCloud(stopwords=stop_words,\n",
        "                  background_color='white',\n",
        "                  width=2500,\n",
        "                  height=1800,\n",
        "                  max_words=10,\n",
        "                  colormap='tab10',\n",
        "                  color_func=lambda *args, **kwargs: cols[i],\n",
        "                  prefer_horizontal=1.0)\n",
        "\n",
        "topics = lda_model_bow.show_topics(formatted=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(20,10), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    fig.add_subplot(ax)\n",
        "    topic_words = dict(topics[i][1])\n",
        "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
        "    plt.gca().imshow(cloud)\n",
        "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
        "    plt.gca().axis('off')\n",
        "\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1jdM_jLhGRF",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. LDA Model with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR08_FBghJAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import TfidfModel\n",
        "\n",
        "tfidf = TfidfModel(bow_corpus)\n",
        "corpus_tfidf = tfidf[bow_corpus]\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "  warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "  lda_model_tfidf = LdaMulticore(corpus_tfidf, num_topics=10, id2word=id2word, passes=2, workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7wUjENSphTF",
        "colab_type": "text"
      },
      "source": [
        "## PyLDAVis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykeasFUrnj6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize the information contained in the topic model\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model_tfidf, corpus_tfidf, dictionary=id2word)\n",
        "vis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKZ-O5DTqrKd",
        "colab_type": "text"
      },
      "source": [
        "## t-SNE Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gDDcrMRqtJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get topic weights\n",
        "topic_weights = []\n",
        "for i, row_list in enumerate(lda_model_bow[bow_corpus]):\n",
        "    topic_weights.append([w for i, w in row_list[0]])\n",
        "\n",
        "# Array of topic weights    \n",
        "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
        "\n",
        "# Keep the well separated points (optional)\n",
        "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
        "\n",
        "# Dominant topic number in each doc\n",
        "topic_num = np.argmax(arr, axis=1)\n",
        "\n",
        "# tSNE Dimension Reduction\n",
        "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
        "tsne_lda = tsne_model.fit_transform(arr)\n",
        "\n",
        "# Plot the Topic Clusters using Bokeh\n",
        "output_notebook()\n",
        "n_topics = 10\n",
        "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
        "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
        "              plot_width=900, plot_height=700)\n",
        "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
        "show(plot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIGWAwEAsDGy",
        "colab_type": "text"
      },
      "source": [
        "## Word Clouds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QM1nqyr8sFCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topics = lda_model_tfidf.show_topics(formatted=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(20,10), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    fig.add_subplot(ax)\n",
        "    topic_words = dict(topics[i][1])\n",
        "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
        "    plt.gca().imshow(cloud)\n",
        "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
        "    plt.gca().axis('off')\n",
        "\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcDmaXNZNgFD",
        "colab_type": "text"
      },
      "source": [
        "# Part 4 Doc2Vec\n",
        "Train two Doc2Vec models upon the vocabularies in Step 2; Visualize your learned word and document embedding space; Collect Doc2Vec representation of each document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbNzF_4EU1bG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qrSxszFU30_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load documents into memory\n",
        "gensim_mapped_documents = [TaggedDocument(d.split(\" \"), [i]) for i, d in enumerate(corpus2_iter)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHuYVFDjNf43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WINDOW_SIZE = 4\n",
        "\n",
        "model = Doc2Vec(window=WINDOW_SIZE, epochs=25, seed=42)\n",
        "model.build_vocab(gensim_mapped_documents)\n",
        "model.train(gensim_mapped_documents, total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYZ8bk9RWgsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc2vec_array = np.array([model.docvecs[i] for i in range(len(gensim_mapped_documents))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wow84p3Ae-Fc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Doing PCA of Doc2Vec data\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quIBt3Cph8-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try standardizing the features?\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "standardized_doc2vec_array = StandardScaler().fit_transform(doc2vec_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKusduMRfAvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "components = pca.fit_transform(standardized_doc2vec_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1FaNnOHfNJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "principle_df = pd.DataFrame(components, columns=[\"c1\", \"c2\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLqWMTryf9sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "principle_df_with_targets = pd.concat([principle_df, pd.DataFrame(dataset_body_only.target, columns=[\"target\"])], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsKL7N17ghd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (16,16))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = dataset_body_only.target_names\n",
        "colors = plt.cm.rainbow(np.linspace(0,1,len(targets)))\n",
        "for target, color in zip(range(len(targets)),colors):\n",
        "    indicesToKeep = principle_df_with_targets['target'] == target\n",
        "    ax.scatter(principle_df_with_targets.loc[indicesToKeep, 'c1'], principle_df_with_targets.loc[indicesToKeep, 'c2'], c = [color], s = 50)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVh_7UpKjgMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hm, that is weird. What about LDA?\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T5NDmbdjmRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=2)\n",
        "components = lda.fit_transform(doc2vec_array, dataset_body_only.target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbJgDp-Gj-az",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda_principle_df = pd.DataFrame(components, columns=[\"c1\", \"c2\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diefcCLokFpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda_principle_df_with_targets = pd.concat([principle_df, pd.DataFrame(dataset_body_only.target, columns=[\"target\"])], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPGQcgGfkHqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (16,16))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component LDA', fontsize = 20)\n",
        "targets = dataset_body_only.target_names\n",
        "colors = plt.cm.rainbow(np.linspace(0,1,len(targets)))\n",
        "for target, color in zip(range(len(targets)),colors):\n",
        "    indicesToKeep = lda_principle_df_with_targets['target'] == target\n",
        "    ax.scatter(lda_principle_df_with_targets.loc[indicesToKeep, 'c1'], lda_principle_df_with_targets.loc[indicesToKeep, 'c2'], c = [color], s = 50)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP9uPHLElPPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hm, maybe just comparing a couple of the targets?\n",
        "# Automobiles vs Baseball vs Hockey\n",
        "\n",
        "doc2vec_array_sports = doc2vec_array[np.logical_or(np.logical_or(dataset_body_only.target == 8, dataset_body_only.target == 9), dataset_body_only.target == 10)]\n",
        "targets_sports = dataset_body_only.target[np.logical_or(np.logical_or(dataset_body_only.target == 8, dataset_body_only.target == 9), dataset_body_only.target == 10)]\n",
        "lda = LinearDiscriminantAnalysis(n_components=2)\n",
        "components = lda.fit_transform(doc2vec_array_sports, targets_sports)\n",
        "principle_df = pd.DataFrame(components, columns=[\"c1\", \"c2\"])\n",
        "principle_df_with_targets = pd.concat([principle_df, pd.DataFrame(targets_sports, columns=[\"target\"])], axis=1)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize = (16,16))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = [dataset_body_only.target_names[8], dataset_body_only.target_names[9], dataset_body_only.target_names[10]]\n",
        "colors = ['r', 'b', 'y']\n",
        "for target, color in zip(range(len(targets)),colors):\n",
        "    indicesToKeep = principle_df_with_targets['target'] == target + 8\n",
        "    ax.scatter(principle_df_with_targets.loc[indicesToKeep, 'c1'], principle_df_with_targets.loc[indicesToKeep, 'c2'], c = [color], s = 50)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MxekkKCs0Ko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inverse LDA transform:\n",
        "def inverse_lda(lda, x):\n",
        "  inv = np.linalg.pinv(lda.scalings_)\n",
        "  return np.dot(x, inv) + lda.xbar_\n",
        "\n",
        "# Direction of new axes:\n",
        "hockey = inverse_lda(lda, np.array([3, 7]))\n",
        "baseball = inverse_lda(lda, np.array([1, -5]))\n",
        "motorcycles = inverse_lda(lda, np.array([-5, 0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYtYRAWEwcQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.wv.similar_by_vector(hockey)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr2nWyhfwgB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.wv.similar_by_vector(baseball)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-R1cYrnxPo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.wv.similar_by_vector(motorcycles)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}